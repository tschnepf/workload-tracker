
services:
  # Production backend with gunicorn
  backend:
    command: gunicorn config.wsgi:application --bind 0.0.0.0:8000 --workers 3 --worker-class gevent --worker-connections 1000 --max-requests 1000 --max-requests-jitter 50 --preload
    environment:
      # Controlled cutover migrations: set to "false" during deploy windows
      # to skip auto-migrate in entrypoint and run migrations manually.
      - RUN_MIGRATIONS_ON_START=${RUN_MIGRATIONS_ON_START:-true}
      # Sentry DSN (set in host env to enable production error reporting)
      - SENTRY_DSN=${SENTRY_DSN}
      - DEBUG=false
      - DJANGO_SETTINGS_MODULE=config.settings
      - SILK_ENABLED=false  # Disable Silk profiling in production
      - ASYNC_JOBS=true
      - REDIS_URL=redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1}
      - BACKUPS_DIR=/backups
      - COOKIE_REFRESH_AUTH=true
      - CSRF_TRUSTED_ORIGINS=${CSRF_TRUSTED_ORIGINS}
    volumes:
      - ./backend:/app:ro  # Read-only mount for security
      - static_volume:/app/staticfiles
      - media_volume:/app/media
      - ./backups:/backups:rw
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    healthcheck:
      test: ["CMD", "curl", "-fsS", "-H", "X-Forwarded-Proto: https", "http://localhost:8000/api/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    build:
      context: ./backend
      dockerfile: ../docker/backend/Dockerfile
      target: production

  # Production frontend - static build
  frontend:
    command: sh -c "cp -r /app/_built/* /app/dist/ 2>/dev/null || true; npx serve -s /app/dist -l 3000"
    environment:
      - NODE_ENV=production
    volumes:
      - frontend_build:/app/dist
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O /dev/null http://localhost:3000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    build:
      context: ./frontend
      dockerfile: ../docker/frontend/Dockerfile
      target: production
      args:
        VITE_API_URL: /api
        VITE_OPENAPI_MIGRATION_ENABLED: "true"
        VITE_COOKIE_REFRESH_AUTH: "true"
        # All endpoint paths migrated to typed client; flags retired

  # Nginx reverse proxy and static file server
  nginx:
    image: nginx:alpine
    container_name: workload-tracker-nginx
    # Optional: only start when COMPOSE_PROFILES includes "proxy"
    profiles: ["proxy"]
    restart: unless-stopped
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/sites-available:/etc/nginx/sites-available:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - static_volume:/var/www/static:ro
      - media_volume:/var/www/media:ro
      - frontend_build:/var/www/frontend:ro
    depends_on:
      backend:
        condition: service_healthy
      frontend:
        condition: service_healthy
    networks:
      - tracker-network
    read_only: true
    tmpfs:
      - /tmp
      - /var/run
      - /var/cache/nginx
    cap_drop:
      - NET_RAW
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis - production configuration
  redis:
    image: redis:7-alpine
    command: redis-server --save 20 1 --loglevel warning --requirepass ${REDIS_PASSWORD:-workload-redis-prod}
    restart: unless-stopped
    volumes:
      - redis_data:/data

  # PostgreSQL - production configuration
  db:
    image: postgres:17.6
    command: >
      postgres
      -c shared_preload_libraries=pg_stat_statements
      -c pg_stat_statements.track=all
      -c pg_stat_statements.max=10000
      -c log_min_duration_statement=5000
      -c log_statement_stats=off
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
    restart: unless-stopped
    volumes:
      - postgres_data_v17:/var/lib/postgresql/data
      - ./backups:/backups  # Mount backup directory

  # Celery worker for background jobs (production)
  worker:
    build:
      context: ./backend
      dockerfile: ../docker/backend/Dockerfile
      target: production
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    command: celery -A config worker -l info --concurrency=2
    environment:
      # Workers never run migrations; backend owns migrations
      - RUN_MIGRATIONS_ON_START=false
      - DEBUG=false
      - DJANGO_SETTINGS_MODULE=config.settings
      - ASYNC_JOBS=true
      - REDIS_URL=redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1}
      - BACKUPS_DIR=/backups
      - COLLECT_STATIC=false
    volumes:
      - ./backend:/app:ro
      - static_volume:/app/staticfiles
      - media_volume:/app/media
      - ./backups:/backups
    depends_on:
      - db
      - redis
    networks:
      - tracker-network

  # Dedicated maintenance worker for DB operations (optional)
  worker_db:
    build:
      context: ./backend
      dockerfile: ../docker/backend/Dockerfile
      target: production
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    command: celery -A config worker -l info --concurrency=1 -Q db_maintenance
    environment:
      # Workers never run migrations; backend owns migrations
      - RUN_MIGRATIONS_ON_START=false
      - DEBUG=false
      - DJANGO_SETTINGS_MODULE=config.settings
      - REDIS_URL=redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1}
      - BACKUPS_DIR=/backups
      - COLLECT_STATIC=false
    volumes:
      - ./backend:/app:ro
      - ./backups:/backups
    depends_on:
      - db
      - redis

  # Optional: Celery beat scheduler (for scheduled backups/retention)
  worker_beat:
    build:
      context: ./backend
      dockerfile: ../docker/backend/Dockerfile
      target: production
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    command: ["celery", "-A", "config", "beat", "-l", "INFO"]
    environment:
      # Workers never run migrations; backend owns migrations
      - RUN_MIGRATIONS_ON_START=false
      - DEBUG=false
      - DJANGO_SETTINGS_MODULE=config.settings
      - ASYNC_JOBS=true
      - REDIS_URL=redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://:${REDIS_PASSWORD:-workload-redis-prod}@redis:6379/1}
      - BACKUPS_DIR=/backups
      - COLLECT_STATIC=false
      # Optional: explicit schedule file (defaults to /tmp/celerybeat-schedule)
      # - CELERY_BEAT_SCHEDULE_FILE=/tmp/celerybeat-schedule
    volumes:
      - ./backend:/app:ro
      - ./backups:/backups
    depends_on:
      - db
      - redis

volumes:
  postgres_data_v17:
    driver: local
  redis_data:
    driver: local
  static_volume:
    driver: local
  media_volume:
    driver: local
  frontend_build:
    driver: local
  # beat_state volume no longer required with /tmp schedule

networks:
  tracker-network:
    driver: bridge
